{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZacCarrico/basic_transformer/blob/main/decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK84vg4_PQoh"
      },
      "source": [
        "# Introduction\n",
        "The goal of this is to train a simple attention-based decoder model using DNA as the input data. The hypothesis is that this model will generate a new DNA sequence with similar codon frequencies to the input DNA.\n",
        "\n",
        "# Background\n",
        "like those used in [Attention is All You Need](https://arxiv.org/abs/1706.03762). This code is a derivative of [Andrej Karpathy's decoder training code](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py[link text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKYLiopNoifH",
        "outputId": "b6815cca-09de-4dfc-8009-82ff562d1332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.646788 M parameters\n",
            "step 0: train loss 1.3830, val loss 1.4116\n",
            "step 500: train loss 1.1361, val loss 1.4802\n",
            "step 999: train loss 1.1192, val loss 1.4472\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5001"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('human_type1_collagen_protein_encoding.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "decoded = decode(m.generate(context, max_new_tokens=5000)[0].tolist())\n",
        "open('generated_dna.txt', 'w').write(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sKg2UgedA1cg"
      },
      "outputs": [],
      "source": [
        "def codon_frequency(dna):\n",
        "  \"\"\"Calculates the frequency of each codon in a DNA sequence.\n",
        "\n",
        "  Args:\n",
        "    dna: A string representing the DNA sequence.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary mapping codons to their frequencies.\n",
        "  \"\"\"\n",
        "\n",
        "  codon_counts = {}\n",
        "  for i in range(0, len(dna), 3):\n",
        "    codon = dna[i:i + 3]\n",
        "    codon_counts[codon] = codon_counts.get(codon, 0) + 1\n",
        "\n",
        "  codon_frequencies = {}\n",
        "  for codon, count in codon_counts.items():\n",
        "    codon_frequencies[codon] = count / len(dna)\n",
        "\n",
        "  return codon_frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('generated_dna.txt', 'r') as f:\n",
        "    decoded = f.readlines()[0]\n",
        "with open('human_type1_collagen_protein_encoding.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "kTdiDfLuCk7j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(input_codon_frequencies)=58\n",
            "len(generated_codon_frequencies)=64\n",
            "{'tcg', 'ccg', 'tag', 'taa', 'tga', 'cta'}\n"
          ]
        }
      ],
      "source": [
        "input_codon_frequencies = codon_frequency(text)\n",
        "generated_codon_frequencies = codon_frequency(decoded)\n",
        "print(f\"{len(input_codon_frequencies)=}\")\n",
        "print(f\"{len(generated_codon_frequencies)=}\")\n",
        "print(set(generated_codon_frequencies.keys()) - set(input_codon_frequencies.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(input_codon_frequencies)=64\n",
            "len(generated_codon_frequencies)=64\n"
          ]
        }
      ],
      "source": [
        "all_codons = []\n",
        "for permutation in itertools.product('atgc', repeat=3):\n",
        "    all_codons.append(''.join(permutation))\n",
        "\n",
        "assert len(all_codons) == 64\n",
        "def add_missing_codons(codons):\n",
        "    for codon in all_codons:\n",
        "        if codon not in codons:\n",
        "            codons[codon] = 0\n",
        "    return codons\n",
        "\n",
        "input_codon_frequencies = add_missing_codons(input_codon_frequencies)\n",
        "generated_codon_frequencies = add_missing_codons(generated_codon_frequencies)\n",
        "print(f\"{len(input_codon_frequencies)=}\")\n",
        "print(f\"{len(generated_codon_frequencies)=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsbxlFHRDIPi",
        "outputId": "cc6c1949-cc67-45fe-d29a-3845292f9bd2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# sorted by key, return a list of tuples\n",
        "input_freq = sorted(input_codon_frequencies.items())\n",
        "gen_freq = sorted(generated_codon_frequencies.items())\n",
        "all_codons.sort()\n",
        "\n",
        "# unpack a list of pairs into two tuples\n",
        "input_freq_codon, input_freq_vals = zip(*input_freq)\n",
        "gen_freq_codon, gen_freq_vals = zip(*gen_freq)\n",
        "\n",
        "diff = []\n",
        "for i in range(len(input_freq_codon)):\n",
        "    diff.append(input_freq_vals[i] - gen_freq_vals[i])\n",
        "\n",
        "\n",
        "plt.figure().set_figwidth(15)\n",
        "plt.bar(all_codons, diff)\n",
        "plt.xticks(rotation=-90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above plot shows the difference in codon frequency between the generated and input sequences. There is a tendency for specific codons in the real (input) DNA to be used to a greater frequency, and for the generated DNA to be uniform in its codons. This preference for specific codons was missed my the model for several codons.\n",
        "\n",
        "When doing a Blast search, the generated DNA doesn't match any known human DNA (not even collagen, the input DNA sequence)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO/9y7pA7ETNRqndIsjTQ4u",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
